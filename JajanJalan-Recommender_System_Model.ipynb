{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLJ6IXyFgQde",
        "outputId": "be3cc209-4a33-4120-9bd0-3c4bda50d7fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oujjEHdEagrR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ed27f9d-7ddc-4776-aa46-a22db34f8b10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflowjs\n",
            "  Downloading tensorflowjs-4.15.0-py3-none-any.whl (89 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/89.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m81.9/89.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flax>=0.7.2 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (0.7.5)\n",
            "Requirement already satisfied: importlib_resources>=5.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (6.1.1)\n",
            "Requirement already satisfied: jax>=0.4.13 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (0.4.20)\n",
            "Requirement already satisfied: jaxlib>=0.4.13 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (0.4.20+cuda12.cudnn89)\n",
            "Requirement already satisfied: tensorflow<3,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (2.15.0)\n",
            "Collecting tensorflow-decision-forests>=1.5.0 (from tensorflowjs)\n",
            "  Downloading tensorflow_decision_forests-1.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six<2,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (0.15.0)\n",
            "Requirement already satisfied: packaging~=23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (23.2)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (1.23.5)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (1.0.7)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (0.1.7)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (0.4.4)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (0.1.45)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (13.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (4.5.0)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from flax>=0.7.2->tensorflowjs) (6.0.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.13->tensorflowjs) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.13->tensorflowjs) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.13->tensorflowjs) (1.11.4)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (16.0.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (67.7.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.4.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.60.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.15.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (1.5.3)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (0.42.0)\n",
            "Collecting wurlitzer (from tensorflow-decision-forests>=1.5.0->tensorflowjs)\n",
            "  Downloading wurlitzer-3.0.3-py3-none-any.whl (7.3 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs) (2.16.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (3.0.1)\n",
            "Requirement already satisfied: chex>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from optax->flax>=0.7.2->tensorflowjs) (0.1.7)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (1.6.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (1.5.8)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2023.3.post1)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.5->optax->flax>=0.7.2->tensorflowjs) (0.1.8)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.5->optax->flax>=0.7.2->tensorflowjs) (0.12.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (1.3.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax>=0.7.2->tensorflowjs) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (2.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.2->tensorflowjs) (2023.6.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.2->tensorflowjs) (3.17.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<3,>=2.13.0->tensorflowjs) (3.2.2)\n",
            "Installing collected packages: wurlitzer, tensorflow-decision-forests, tensorflowjs\n",
            "Successfully installed tensorflow-decision-forests-1.8.1 tensorflowjs-4.15.0 wurlitzer-3.0.3\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import numpy.ma as ma\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tabulate\n",
        "from collections import defaultdict\n",
        "import csv\n",
        "import numpy as np\n",
        "from numpy import genfromtxt\n",
        "import tabulate\n",
        "from IPython.display import HTML\n",
        "import pickle\n",
        "from IPython.display import display\n",
        "!pip install tensorflowjs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "pof6N8DdWgce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "item_train = genfromtxt('/content/drive/Shareddrives/BANGKIT 2023 CAPSTONE/CAPSTONE/DATASET/experimental_Seller_Features - Sheet1.csv', delimiter=',')\n",
        "user_train = genfromtxt('/content/drive/Shareddrives/BANGKIT 2023 CAPSTONE/CAPSTONE/DATASET/experimental_User_Features - Sheet2.csv', delimiter=',')\n",
        "y_train = genfromtxt('/content/drive/Shareddrives/BANGKIT 2023 CAPSTONE/CAPSTONE/DATASET/y_train - Sheet2.csv', delimiter=',')\n",
        "with open('/content/drive/Shareddrives/BANGKIT 2023 CAPSTONE/CAPSTONE/DATASET/item_train header.txt', newline='') as f: #csv reader handles quoted strings better\n",
        "  item_features = list(csv.reader(f))[0]\n",
        "with open('/content/drive/Shareddrives/BANGKIT 2023 CAPSTONE/CAPSTONE/DATASET/user_train header.txt', newline='') as f:\n",
        "  user_features = list(csv.reader(f))[0]\n",
        "\n",
        "#item_vecs = genfromtxt('/content/drive/MyDrive/CAPSTONE/DATASET/Item Vecs - Sheet1.csv', delimiter=',')\n",
        "seller_vecs = genfromtxt('/content/drive/Shareddrives/BANGKIT 2023 CAPSTONE/CAPSTONE/DATASET/experimental_Item Vecs - Sheet1.csv', delimiter=',')\n",
        "\n",
        "menu_dict = defaultdict(dict)\n",
        "count = 0\n",
        "#    with open('./data/movies.csv', newline='') as csvfile:\n",
        "with open('/content/drive/Shareddrives/BANGKIT 2023 CAPSTONE/CAPSTONE/DATASET/menu_dict - Sheet1.csv', newline='') as csvfile:\n",
        "  reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
        "  for line in reader:\n",
        "      if count == 0:\n",
        "          count += 1  #skip header\n",
        "          #print(line) print\n",
        "      else:\n",
        "          count += 1\n",
        "          seller_id = int(line[0])\n",
        "          menu_dict[seller_id][\"nama_penjual\"] = line[1]\n",
        "          menu_dict[seller_id][\"menu\"] = line[2]"
      ],
      "metadata": {
        "id": "xAkpGwBVwquO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_user_features = user_train.shape[1] - 3  # remove userid, rating count and ave rating during training\n",
        "num_item_features = item_train.shape[1] - 2  # remove seller id and 'none' column at train time\n",
        "uvs = 3  # user genre vector start\n",
        "ivs = 3  # item genre vector start\n",
        "u_s = 3  # start of columns to use in training, user\n",
        "i_s = 2  # start of columns to use in training, items\n",
        "print(f\"Number of training vectors: {len(item_train)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDDACTzJawPh",
        "outputId": "47492d05-f325-4641-b67c-a7a66aa1e780"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training vectors: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizing Data"
      ],
      "metadata": {
        "id": "wMtDL_vZifx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create HTML tables for visualization\n",
        "def create_html_table(data, header, max_rows=5):\n",
        "    html_table = \"<table>\\n<thead>\\n<tr>\"\n",
        "    html_table += \"\".join([f\"<th>{h}</th>\" for h in header])\n",
        "    html_table += \"</tr>\\n</thead>\\n<tbody>\\n\"\n",
        "    for row in data[:max_rows]:\n",
        "        html_table += \"<tr>\"\n",
        "        html_table += \"\".join([f\"<td>{col}</td>\" for col in row])\n",
        "        html_table += \"</tr>\\n\"\n",
        "    html_table += \"</tbody>\\n</table>\"\n",
        "    return html_table"
      ],
      "metadata": {
        "id": "fuJoJZvor6Ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "item_table_html = create_html_table(item_train, item_features, max_rows=5)\n",
        "HTML(item_table_html)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "eAmunR82vphe",
        "outputId": "e6314fff-6f9c-46c3-dd08-0138d2b78382"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table>\n",
              "<thead>\n",
              "<tr><th>seller_id</th><th>none</th><th>ave_rating</th><th>nasi_goreng</th><th>mie_ayam</th><th>tahu_tek</th><th>batagor</th><th>pentol</th><th>kacang_hijau</th><th>es_dawet</th><th>es_oyen</th><th>sate</th><th>siomay</th><th>es_campur</th><th>soto</th><th>seblak</th><th>rujak</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>1.0</td><td>2000.0</td><td>4.6</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr>\n",
              "<tr><td>2.0</td><td>2000.0</td><td>2.6</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr>\n",
              "<tr><td>3.0</td><td>2000.0</td><td>3.8</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr>\n",
              "<tr><td>4.0</td><td>2000.0</td><td>4.8</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr>\n",
              "<tr><td>5.0</td><td>2000.0</td><td>2.2</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr>\n",
              "</tbody>\n",
              "</table>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_table_html = create_html_table(user_train, user_features, max_rows=5)\n",
        "HTML(user_table_html)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "VViRdVZzstgb",
        "outputId": "7eeb2dbc-1be7-4f4e-c6a4-1258b8b7db53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table>\n",
              "<thead>\n",
              "<tr><th>user_id</th><th>rating_count</th><th>rating_ave</th><th>nasi_goreng</th><th>mie_ayam</th><th>tahu_tek</th><th>batagor</th><th>pentol</th><th>kacang_hijau</th><th>es_dawet</th><th>es_oyen</th><th>sate</th><th>siomay</th><th>es_campur</th><th>soto</th><th>seblak</th><th>rujak</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td>1.0</td><td>8.0</td><td>1.5</td><td>4.5</td><td>2.0</td><td>0.0</td><td>0.0</td><td>4.0</td><td>4.0</td><td>3.5</td><td>3.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr>\n",
              "<tr><td>2.0</td><td>15.0</td><td>3.185714286</td><td>3.4</td><td>1.0</td><td>0.0</td><td>5.0</td><td>1.2</td><td>4.8</td><td>4.8</td><td>4.8</td><td>2.0</td><td>2.0</td><td>3.4</td><td>3.8</td><td>4.8</td><td>3.6</td></tr>\n",
              "<tr><td>3.0</td><td>20.0</td><td>2.7</td><td>3.8</td><td>1.8</td><td>3.8</td><td>4.0</td><td>3.4</td><td>1.2</td><td>2.4</td><td>4.2</td><td>2.2</td><td>0.2</td><td>1.8</td><td>4.4</td><td>0.2</td><td>4.4</td></tr>\n",
              "<tr><td>4.0</td><td>16.0</td><td>3.057142857</td><td>2.8</td><td>1.8</td><td>4.8</td><td>3.6</td><td>4.4</td><td>3.4</td><td>2.8</td><td>4.6</td><td>3.4</td><td>3.2</td><td>1.2</td><td>2.6</td><td>2.6</td><td>1.6</td></tr>\n",
              "<tr><td>5.0</td><td>20.0</td><td>2.671428571</td><td>4.0</td><td>1.4</td><td>2.6</td><td>3.6</td><td>2.4</td><td>2.6</td><td>2.0</td><td>5.0</td><td>1.4</td><td>1.2</td><td>1.0</td><td>0.8</td><td>4.6</td><td>4.8</td></tr>\n",
              "</tbody>\n",
              "</table>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "CujVhummipaC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scaling**"
      ],
      "metadata": {
        "id": "Nuw6-aUBiu6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# scale training data\n",
        "item_train_unscaled = item_train\n",
        "user_train_unscaled = user_train\n",
        "y_train_unscaled    = y_train\n",
        "\n",
        "scalerItem = StandardScaler()\n",
        "scalerItem.fit(item_train)\n",
        "item_train = scalerItem.transform(item_train)\n",
        "\n",
        "scalerUser = StandardScaler()\n",
        "scalerUser.fit(user_train)\n",
        "user_train = scalerUser.transform(user_train)\n",
        "\n",
        "scalerTarget = MinMaxScaler((-1, 1))\n",
        "scalerTarget.fit(y_train.reshape(-1, 1))\n",
        "y_train = scalerTarget.transform(y_train.reshape(-1, 1))\n",
        "#ynorm_test = scalerTarget.transform(y_test.reshape(-1, 1))\n",
        "\n",
        "print(np.allclose(item_train_unscaled, scalerItem.inverse_transform(item_train)))\n",
        "print(np.allclose(user_train_unscaled, scalerUser.inverse_transform(user_train)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRvrtqBxwqr2",
        "outputId": "88c4c994-137f-4572-8d01-6930d4dd1eaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train-Test Splitting**"
      ],
      "metadata": {
        "id": "IrZrtPqiizOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "item_train, item_test = train_test_split(item_train, train_size=0.80, shuffle=True, random_state=1)\n",
        "user_train, user_test = train_test_split(user_train, train_size=0.80, shuffle=True, random_state=1)\n",
        "y_train, y_test       = train_test_split(y_train,    train_size=0.80, shuffle=True, random_state=1)\n",
        "print(f\"seller/item training data shape: {item_train.shape}\")\n",
        "print(f\"seller/item test data shape: {item_test.shape}\")"
      ],
      "metadata": {
        "id": "u72b8T6ZFsXN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "328a08f9-5946-40e8-90a1-7eebd19964a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seller/item training data shape: (800, 17)\n",
            "seller/item test data shape: (200, 17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning Model"
      ],
      "metadata": {
        "id": "-FR55JdEjMMO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Model Architecture**"
      ],
      "metadata": {
        "id": "S_jMWp9ri7Hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_outputs = 32\n",
        "tf.random.set_seed(1)\n",
        "user_NN = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(units=256, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=128, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=num_outputs)\n",
        "])\n",
        "\n",
        "item_NN = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(units=256, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=128, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=num_outputs)\n",
        "])"
      ],
      "metadata": {
        "id": "-rceBjCfJn7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the user input and point to the base network\n",
        "input_user = tf.keras.layers.Input(shape=(num_user_features))\n",
        "vu = user_NN(input_user)\n",
        "vu = tf.linalg.l2_normalize(vu, axis=1)\n",
        "\n",
        "# create the item input and point to the base network\n",
        "input_item = tf.keras.layers.Input(shape=(num_item_features))\n",
        "vm = item_NN(input_item)\n",
        "vm = tf.linalg.l2_normalize(vm, axis=1)"
      ],
      "metadata": {
        "id": "FO1N48XEJy-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the dot product of the two vectors vu and vm\n",
        "output = tf.keras.layers.Dot(axes=1)([vu, vm])\n",
        "\n",
        "# specify the inputs and output of the model\n",
        "model = tf.keras.Model([input_user, input_item], output)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxjCUEBzJ6Rj",
        "outputId": "6b4a5f49-896a-40e3-8fe9-9c914b13010d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 14)]                 0         []                            \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, 15)]                 0         []                            \n",
            "                                                                                                  \n",
            " sequential (Sequential)     (None, 32)                   40864     ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " sequential_1 (Sequential)   (None, 32)                   41120     ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " tf.math.l2_normalize (TFOp  (None, 32)                   0         ['sequential[0][0]']          \n",
            " Lambda)                                                                                          \n",
            "                                                                                                  \n",
            " tf.math.l2_normalize_1 (TF  (None, 32)                   0         ['sequential_1[0][0]']        \n",
            " OpLambda)                                                                                        \n",
            "                                                                                                  \n",
            " dot (Dot)                   (None, 1)                    0         ['tf.math.l2_normalize[0][0]',\n",
            "                                                                     'tf.math.l2_normalize_1[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 81984 (320.25 KB)\n",
            "Trainable params: 81984 (320.25 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(1)\n",
        "cost_fn = tf.keras.losses.MeanSquaredError()\n",
        "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
        "model.compile(optimizer=opt,\n",
        "              loss=cost_fn)"
      ],
      "metadata": {
        "id": "gZO4So8cKB_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training**"
      ],
      "metadata": {
        "id": "EDb2somKjZCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(1)\n",
        "model.fit([user_train[:, u_s:], item_train[:, i_s:]], y_train, epochs=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXqRe_AOKFrq",
        "outputId": "d87ee049-b69c-4a5f-ca53-6b245ab538cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "25/25 [==============================] - 3s 11ms/step - loss: 0.1375\n",
            "Epoch 2/30\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.1036\n",
            "Epoch 3/30\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.0865\n",
            "Epoch 4/30\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.0688\n",
            "Epoch 5/30\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.0625\n",
            "Epoch 6/30\n",
            "25/25 [==============================] - 0s 12ms/step - loss: 0.0557\n",
            "Epoch 7/30\n",
            "25/25 [==============================] - 0s 13ms/step - loss: 0.0387\n",
            "Epoch 8/30\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.0330\n",
            "Epoch 9/30\n",
            "25/25 [==============================] - 0s 11ms/step - loss: 0.0241\n",
            "Epoch 10/30\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.0169\n",
            "Epoch 11/30\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.0158\n",
            "Epoch 12/30\n",
            "25/25 [==============================] - 0s 15ms/step - loss: 0.0115\n",
            "Epoch 13/30\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.0100\n",
            "Epoch 14/30\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.0074\n",
            "Epoch 15/30\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.0058\n",
            "Epoch 16/30\n",
            "25/25 [==============================] - 0s 14ms/step - loss: 0.0056\n",
            "Epoch 17/30\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.0051\n",
            "Epoch 18/30\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.0045\n",
            "Epoch 19/30\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.0039\n",
            "Epoch 20/30\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.0033\n",
            "Epoch 21/30\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.0031\n",
            "Epoch 22/30\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.0026\n",
            "Epoch 23/30\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.0027\n",
            "Epoch 24/30\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.0022\n",
            "Epoch 25/30\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.0026\n",
            "Epoch 26/30\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.0026\n",
            "Epoch 27/30\n",
            "25/25 [==============================] - 0s 8ms/step - loss: 0.0021\n",
            "Epoch 28/30\n",
            "25/25 [==============================] - 0s 6ms/step - loss: 0.0022\n",
            "Epoch 29/30\n",
            "25/25 [==============================] - 0s 9ms/step - loss: 0.0020\n",
            "Epoch 30/30\n",
            "25/25 [==============================] - 0s 7ms/step - loss: 0.0021\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x792cb43439d0>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate([user_test[:, u_s:], item_test[:, i_s:]], y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLlKGenkKN38",
        "outputId": "0eabfccb-3cc0-438e-94fe-80729f6be1ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7/7 [==============================] - 0s 4ms/step - loss: 0.0880\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.08803819864988327"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction"
      ],
      "metadata": {
        "id": "YctAPPZLR4uS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prediction for New User**"
      ],
      "metadata": {
        "id": "2EBbxga3SH2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_user_vecs(user_vec, num_items):\n",
        "    \"\"\" given a user vector return:\n",
        "        user predict maxtrix to match the size of item_vecs \"\"\"\n",
        "    user_vecs = np.tile(user_vec, (num_items, 1))\n",
        "    return user_vecs\n",
        "\n",
        "def print_pred_movies(y_p, item, menu_dict, maxcount):\n",
        "    \"\"\" print results of prediction of a new user. inputs are expected to be in\n",
        "        sorted order, unscaled. \"\"\"\n",
        "    count = 0\n",
        "    disp = [[\"y_p\", \"seller_id\", \"rating_ave\", \"nama_penjual\", \"menu\"]]\n",
        "\n",
        "    for i in range(0, y_p.shape[0]):\n",
        "        if count == maxcount:\n",
        "            break\n",
        "        count += 1\n",
        "        seller_id = item[i, 0].astype(int)\n",
        "        disp.append([np.around(y_p[i, 0], 1), item[i, 0].astype(int), np.around(item[i, 2].astype(float), 1),\n",
        "                     menu_dict[seller_id]['nama_penjual'], menu_dict[seller_id]['menu']])\n",
        "\n",
        "    table = tabulate.tabulate(disp, tablefmt='html', headers=\"firstrow\")\n",
        "    return table"
      ],
      "metadata": {
        "id": "Z3Oz9h8Kf3yD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_user_id = 1001 #user_id\n",
        "new_rating_count = 30 #berapa kali menilai\n",
        "new_rating_ave = 3.75 #rata-rata dari seluruh menu\n",
        "nasi_goreng = 3.00 #rata-rata dari seluruh jenis makanan yang sama atau skor preferensi\n",
        "mie_ayam = 3.00\n",
        "tahu_tek = 3.00\n",
        "batagor = 3.00\n",
        "pentol = 4.00\n",
        "kacang_hijau = 5.00\n",
        "es_dawet = 3.00\n",
        "es_oyen = 3.00\n",
        "sate = 3.00\n",
        "siomay = 3.00\n",
        "es_campur = 3.00\n",
        "soto = 3.00\n",
        "seblak = 3.00\n",
        "rujak = 3.00\n",
        "\n",
        "user_vec = np.array([[new_user_id,new_rating_count,new_rating_ave,nasi_goreng,mie_ayam,tahu_tek,batagor,pentol,kacang_hijau,es_dawet,es_oyen,sate,siomay,es_campur,soto,seblak,rujak]])"
      ],
      "metadata": {
        "id": "a5vLyzVPKXdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate and replicate the user vector to match the number movies in the data set.\n",
        "user_vecs = gen_user_vecs(user_vec,len(seller_vecs))\n",
        "\n",
        "# scale our user and item vectors\n",
        "suser_vecs = scalerUser.transform(user_vecs)\n",
        "sseller_vecs = scalerItem.transform(seller_vecs)\n",
        "\n",
        "# make a prediction\n",
        "y_p = model.predict([suser_vecs[:, u_s:], sseller_vecs[:, i_s:]])\n",
        "\n",
        "# unscale y prediction\n",
        "y_pu = scalerTarget.inverse_transform(y_p)\n",
        "\n",
        "# sort the results, highest prediction first\n",
        "sorted_index = np.argsort(-y_pu,axis=0).reshape(-1).tolist()  #negate to get largest rating first\n",
        "sorted_ypu   = y_pu[sorted_index]\n",
        "sorted_items = seller_vecs[sorted_index]  #using unscaled vectors for display\n",
        "\n",
        "HTML(print_pred_movies(sorted_ypu, sorted_items, menu_dict, maxcount = 10))"
      ],
      "metadata": {
        "id": "p55CNH4tdzB9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "86dc8c8b-6e3e-452d-876c-ba2c5f9ebcdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/12 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table>\n",
              "<thead>\n",
              "<tr><th style=\"text-align: right;\">  y_p</th><th style=\"text-align: right;\">  seller_id</th><th style=\"text-align: right;\">  rating_ave</th><th>nama_penjual        </th><th>menu        </th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "<tr><td style=\"text-align: right;\">  4.6</td><td style=\"text-align: right;\">        125</td><td style=\"text-align: right;\">         5  </td><td>es_oyen Bu V N      </td><td>es_oyen     </td></tr>\n",
              "<tr><td style=\"text-align: right;\">  4.6</td><td style=\"text-align: right;\">         78</td><td style=\"text-align: right;\">         5  </td><td>es_oyen Bu X J      </td><td>es_oyen     </td></tr>\n",
              "<tr><td style=\"text-align: right;\">  4.6</td><td style=\"text-align: right;\">        166</td><td style=\"text-align: right;\">         4.6</td><td>es_oyen Bu H F      </td><td>es_oyen     </td></tr>\n",
              "<tr><td style=\"text-align: right;\">  4.6</td><td style=\"text-align: right;\">        346</td><td style=\"text-align: right;\">         4.6</td><td>es_oyen Bu G P      </td><td>es_oyen     </td></tr>\n",
              "<tr><td style=\"text-align: right;\">  4.5</td><td style=\"text-align: right;\">        320</td><td style=\"text-align: right;\">         4.4</td><td>es_oyen Pak R J     </td><td>es_oyen     </td></tr>\n",
              "<tr><td style=\"text-align: right;\">  4.5</td><td style=\"text-align: right;\">        287</td><td style=\"text-align: right;\">         4.4</td><td>es_oyen Bu B H      </td><td>es_oyen     </td></tr>\n",
              "<tr><td style=\"text-align: right;\">  4.5</td><td style=\"text-align: right;\">        266</td><td style=\"text-align: right;\">         4.4</td><td>es_oyen Bu G N      </td><td>es_oyen     </td></tr>\n",
              "<tr><td style=\"text-align: right;\">  4.5</td><td style=\"text-align: right;\">        262</td><td style=\"text-align: right;\">         5  </td><td>es_campur Bu O T    </td><td>es_campur   </td></tr>\n",
              "<tr><td style=\"text-align: right;\">  4.5</td><td style=\"text-align: right;\">         69</td><td style=\"text-align: right;\">         5  </td><td>kacang_hijau Pak V T</td><td>kacang_hijau</td></tr>\n",
              "<tr><td style=\"text-align: right;\">  4.5</td><td style=\"text-align: right;\">        270</td><td style=\"text-align: right;\">         5  </td><td>kacang_hijau Bu T C </td><td>kacang_hijau</td></tr>\n",
              "</tbody>\n",
              "</table>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"path/to/save/model\")"
      ],
      "metadata": {
        "id": "mKjGyavk3Bsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tensorflowjs_converter --input_format=tf_saved_model \\\n",
        "                       --output_node_names='dot_1' \\\n",
        "                       path/to/saved_model \\\n",
        "                       path/to/tfjs_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGvJ30RZ1S4F",
        "outputId": "3d3a56c5-f194-495b-8697-c8d3f38fd3cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-16 13:42:54.447593: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-16 13:42:54.447687: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-16 13:42:54.451489: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-16 13:42:56.448513: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "model_json = model.to_json()\n",
        "with open(\"path/to/drive/model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "eyUrBF4i6Bo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from tensorflow.keras.models import load_model"
      ],
      "metadata": {
        "id": "A_O7FVmcgn9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "from flask import Flask, request, jsonify\n",
        "\n",
        "app = Flask(__name__)\n",
        "model = load_model(\"path/to/save/model\")\n",
        "\n",
        "@app.route(\"/predict\", methods=[\"POST\"])\n",
        "\n",
        "def predict(input_data):\n",
        "    data = request.json\n",
        "    # Perform inference using the loaded model\n",
        "    #seller_vecs = genfromtxt('/content/drive/MyDrive/CAPSTONE/DATASET/experimental_Item Vecs - Sheet1.csv', delimiter=',')\n",
        "    user_unscaled = data\n",
        "\n",
        "    #scalerItem = StandardScaler()\n",
        "    #scalerItem.fit(seller_vecs)\n",
        "    #seller_vecs_scaled = scalerItem.transform(seller_vecs)\n",
        "\n",
        "    scalerUser = StandardScaler()\n",
        "    scalerUser.fit(user_unscaled)\n",
        "    user_vecs_scaled = scalerUser.transform(user_unscaled)\n",
        "\n",
        "    result = model.predict([user_vecs_scaled[:, 3:], seller_vecs_scaled[:, 2:]])\n",
        "    return jsonify({\"prediction\": result.tolist()})\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run(host=\"0.0.0.0\", port=5000)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "w61R-sc5gW2l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "fe7ba789-3bdf-4b67-b660-f87bcbe5e0a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-23-94f9921c016f>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    input_data=np.array([[new_user_id,new_rating_count,new_rating_ave,nasi_goreng,mie_ayam,\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}